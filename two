from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.email_operator import EmailOperator
from datetime import datetime
import monetdblite

# Define default_args
default_args = {
    'owner': 'your_name',
    'start_date': datetime(2023, 10, 30),
    'retries': 1,
    'email': ['your_email@example.com'],  # Add your email address here
    'email_on_failure': True,
}

# Create a DAG
dag = DAG(
    'example_dag',
    default_args=default_args,
    schedule_interval=None,  # You can set a schedule_interval as needed
    catchup=False,
    tags=['example'],
)

# Define 8 Python functions as tasks
def task_1():
    print("Task 1")

def task_2():
    print("Task 2")

def task_3():
    print("Task 3")

def task_4():
    print("Task 4")

def task_5():
    print("Task 5")

def task_6():
    print("Task 6")

def task_7():
    # Connect to MonetDB and execute the first query for model_name
    connection = monetdblite.connect('your_monetdb_database')
    cursor = connection.cursor()
    cursor.execute('SELECT your_model_name FROM your_table')
    
    # Fetch the result and store it in a variable
    model_name_result = cursor.fetchall()
    
    # Execute the second query for job_log_id
    cursor.execute('SELECT your_column FROM your_table')
    
    # Fetch the result and store it in another variable
    job_log_id_result = cursor.fetchall()
    
    # Close the connection
    connection.close()
    
    # Print the results (optional)
    print(f"Task 7 Model Name Result: {model_name_result}")
    print(f"Task 7 Job Log ID Result: {job_log_id_result}")
    
    # Return the results to be passed to the next task
    return model_name_result, job_log_id_result

# Create 8 PythonOperator tasks
task1 = PythonOperator(
    task_id='task_1',
    python_callable=task_1,
    dag=dag,
)

task2 = PythonOperator(
    task_id='task_2',
    python_callable=task_2,
    dag=dag,
)

task3 = PythonOperator(
    task_id='task_3',
    python_callable=task_3,
    dag=dag,
)

task4 = PythonOperator(
    task_id='task_4',
    python_callable=task_4,
    dag=dag,
)

task5 = PythonOperator(
    task_id='task_5',
    python_callable=task_5,
    dag=dag,
)

task6 = PythonOperator(
    task_id='task_6',
    python_callable=task_6,
    dag=dag,
)

task7 = PythonOperator(
    task_id='task_7',
    python_callable=task_7,
    provide_context=True,  # Pass the context to access outputs of previous tasks
    dag=dag,
)

# Add an email task to notify on failure
email_task = EmailOperator(
    task_id='email_notification',
    to='your_email@example.com',  # Add your email address here
    subject='Airflow Task Failed',
    provide_context=True,  # Pass the context to access outputs of previous tasks
    html_content='<html><head><style>table {border-collapse: collapse; border: 3px solid black; width: 50%;} td {width: 10%; height: 2em; border: 2px solid #ccc;} th {background-color: green; color: white;}</style></head><body><p>Hi Team,</p><p>DAG completed</p><table><tr><th>INDUSTRY</th><th>STACK</th><th>Model Name</th><th>Job log ID</th></tr><tr><td>TEC_WTD</td><td>A</td><td>{{ ti.xcom_pull(task_ids="task_7")[0][0] }}</td><td>{{ ti.xcom_pull(task_ids="task_7")[1][0] }}</td></tr></table></body></html>',
    dag=dag,
)

# Set the email task to trigger on failure of any of the tasks
task1 >> task2 >> task3 >> task4 >> task5 >> task6 >> task7 >> email_task
